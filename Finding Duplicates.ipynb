{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4146cf4a-80a9-4539-8eb1-669c6847481f",
   "metadata": {},
   "source": [
    "# Finding Duplicates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56505f-7498-40fd-862b-7aaf9bd71069",
   "metadata": {},
   "source": [
    "##### Data wrangling is a critical step in preparing datasets for analysis, and handling duplicates plays a key role in ensuring data accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131edae-7b57-4a3f-8826-8284dce9382b",
   "metadata": {},
   "source": [
    "###### Install the needed library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091a00d-b237-4684-94b0-b11f3c972d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfcdd00-4bf3-4ed3-9ed4-a9025fc9170f",
   "metadata": {},
   "source": [
    "###### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5076f0-4584-4a1d-aa65-cb2c38ee2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9de3f9-8026-4d97-8b3d-c29986777910",
   "metadata": {},
   "source": [
    "## Load the dataset into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b7879-b107-4e4d-8e8a-a6db4071d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset directly from the URL\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VYPrOu0Vs3I0hKLLjiPGrA/survey-data-with-duplicate.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221f2d1-4456-4aba-9983-607addc31450",
   "metadata": {},
   "source": [
    "### Identify and Analyze Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae42409-7c63-4673-bbb5-fa183f365c72",
   "metadata": {},
   "source": [
    "#### Task 1: Identify Duplicate Rows\n",
    "##### 1:Count the number of duplicate rows in the dataset.\n",
    "##### 2:Display the first few duplicate rows to understand their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba07d9b-17ab-4cce-9d5e-bf3d2a3ee9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows:{duplicate_count}\")\n",
    "# 2. Display the first few duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"first few duplicate rows:\")\n",
    "print(duplicate_rows.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18073132-7ddd-4e01-bb17-63f8306df8af",
   "metadata": {},
   "source": [
    "### Task 2: Analyze Characteristics of DuplicatesÂ¶\n",
    "##### 1-Identify duplicate rows based on selected columns such as MainBranch, Employment, and RemoteWork. Analyse which columns frequently contain identical values within these duplicate rows.\n",
    "##### 2-Analyse the characteristics of rows that are duplicates based on a subset of columns, such as MainBranch, Employment, and RemoteWork. Determine which columns frequently have identical values across these rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fbb85a-31f3-4091-85ec-037724bce8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "# Step 1: Find duplicate rows\n",
    "subset_cols = ['MainBranch', 'Age', 'RemoteWork']\n",
    "dups = df[df.duplicated(subset=subset_cols, keep=False)]\n",
    "\n",
    "report = {\n",
    "    col: (\n",
    "        \"All values identical\" if dups[col].nunique(dropna=False) == 1\n",
    "        else f\"Low variation ({dups[col].nunique()} unique values)\" if dups[col].nunique() <= 5\n",
    "        else f\"High variation ({dups[col].nunique()} unique values)\"\n",
    "    )\n",
    "    for col in df.columns\n",
    "}\n",
    "\n",
    "similarity_df = pd.DataFrame(report.items(), columns=['Column', 'DuplicateRowSimilarity']).sort_values('DuplicateRowSimilarity')\n",
    "similarity_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64863f8-7a49-41a1-ae01-e2f2004c1c5c",
   "metadata": {},
   "source": [
    "### Task 3: Visualize Duplicates Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65598230-3212-4dfb-b09a-9fc5d125de56",
   "metadata": {},
   "source": [
    "##### 1-Create visualizations to show the distribution of duplicates across different categories.\n",
    "##### 2-Use bar charts or pie charts to represent the distribution of duplicates by Country and Employment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b9233-59a6-4a46-abaa-e4ed5bd0d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "# Find all duplicate rows (include all duplicates, not just the later ones)\n",
    "duplicates = df[df.duplicated(keep=False)]\n",
    "\n",
    "# -------------------- Bar Chart: Duplicates by Country --------------------\n",
    "country_counts = duplicates['Country'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(country_counts.index, country_counts.values, color='skyblue')\n",
    "plt.title('Duplicate Rows by Country', fontsize=14)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Number of Duplicates', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------- Pie Chart: Duplicates by Employment --------------------\n",
    "# Count duplicates by employment\n",
    "employment_counts = duplicates['Employment'].value_counts()\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(16, 6))\n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    employment_counts,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=plt.cm.tab10.colors,\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "\n",
    "# Legend on the side (kinare)\n",
    "plt.legend(\n",
    "    wedges,\n",
    "    employment_counts.index,\n",
    "    title=\"Employment\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    "    fontsize=9\n",
    ")\n",
    "\n",
    "plt.title('Duplicates by Employment')\n",
    "plt.axis('equal')  # To make it circular\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a97e4-29ca-4485-aff1-683e6d16d896",
   "metadata": {},
   "source": [
    "## Task 4: Strategic Removal of Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0956800-be3c-4639-9603-3b2e35cacbe9",
   "metadata": {},
   "source": [
    "#### Decide which columns are critical for defining uniqueness in the dataset.\n",
    "#### Remove duplicates based on a subset of columns if complete row duplication is not a good criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9c50a-8f02-4e06-a3b4-5250c96379b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicates based on important identifying fields\n",
    "duplicates = df[df.duplicated(subset=['Age', 'Country', 'EdLevel', 'YearsCode', 'DevType'], keep=False)]\n",
    "print(\"Potential Duplicates Found:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates\n",
    "df_unique = df.drop_duplicates(subset=['Age', 'Country', 'EdLevel', 'YearsCode', 'DevType'], keep='first')\n",
    "\n",
    "# Check how many rows were removed\n",
    "print(f\"\\nDuplicates removed: {len(df) - len(df_unique)}\")\n",
    "\n",
    "# Optionally display cleaned data\n",
    "display(df_unique.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89cec4e-2655-4ffc-a2c6-a5f762ea016c",
   "metadata": {},
   "source": [
    "## Verify and Document Duplicate Removal Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da9770-b7a3-44a8-a1f3-654eb8c7859c",
   "metadata": {},
   "source": [
    "#### Task 5: Documentation\n",
    "##### 1: Document the process of identifying and removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a57dc6-69c4-4774-8c53-13bad2309ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your explanation here\n",
    "1: Document the process of identifying and removing duplicates\n",
    "The following steps were used in your notebook to identify and remove duplicate records from the dataset:\n",
    "\n",
    "Step 1: Load the dataset\n",
    "The dataset was read directly from a cloud URL using pandas.read_csv() and loaded into a DataFrame:\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VYPrOu0Vs3I0hKLLjiPGrA/survey-data-with-duplicate.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "Step 2: Identify duplicate rows\n",
    "You counted total duplicate rows in the entire dataset:\n",
    "duplicate_count = df.duplicated().sum()\n",
    "Step 3: Display duplicate rows\n",
    "The first few duplicate rows were printed for manual inspection:\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "display(duplicate_rows.head())\n",
    "Step 4: Identify duplicates based on specific columns\n",
    "To refine the detection, you selected a subset of columns ('MainBranch', 'Age', 'RemoteWork') and checked for duplicated rows only within that subset:\n",
    "subset_cols = ['MainBranch', 'Age', 'RemoteWork']\n",
    "dups = df[df.duplicated(subset=subset_cols, keep=False)]\n",
    "Step 5: Analyze similarities in duplicate groups\n",
    "You created a summary that classifies each column based on how similar its values are among duplicates:\n",
    "\n",
    "report = {\n",
    "    col: (\n",
    "        \"All values identical\" if dups[col].nunique(dropna=False) == 1\n",
    "        else f\"Low variation ({dups[col].nunique()} unique values)\" if dups[col].nunique() <= 5\n",
    "        else f\"High variation ({dups[col].nunique()} unique values)\"\n",
    "    )\n",
    "    for col in df.columns\n",
    "}\n",
    "similarity_df = pd.DataFrame(report.items(), columns=['Column', 'DuplicateRowSimilarity'])\n",
    "\n",
    "\n",
    "Step 6: Remove duplicates\n",
    "Though not explicitly shown in the preview, typically you would clean the data by:\n",
    "\n",
    "df_cleaned = df.drop_duplicates(subset=subset_cols, keep='first')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e554e9-5bac-42b0-a6ea-c4c6208107e6",
   "metadata": {},
   "source": [
    "##### Explain the reasoning behind selecting specific columns for identifying and removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f6f44-2c07-4ba2-b8c7-f82b605634e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your explanation here\n",
    "2: Explain the reasoning behind selecting specific columns for identifying and removing duplicates\n",
    "The subset used for identifying duplicates included:\n",
    "\n",
    "subset_cols = ['Age', 'Country', 'EdLevel', 'YearsCode', 'DevType']\n",
    "The reasoning for selecting these specific columns is as follows:\n",
    "\n",
    "Relevance to respondent profile: These columns are indicative of a respondentâs work status and demographic (e.g., whether they are working remotely, their age, and how they identified themselves professionally). Duplicate responses with the same values in these key fields are likely repeated entries from the same individual.\n",
    "\n",
    "Simplification of comparison: Focusing on three core features simplifies the identification process without removing valid variations in less critical fields like Gender, Country, or SurveyEase.\n",
    "\n",
    "Avoiding over-deletion: Not all duplicate rows in a dataset are exactly the same across all columns. By narrowing the check to key identifying attributes, you reduce the risk of removing distinct entries that only share values in unrelated fields.\n",
    "\n",
    "Strategic cleanup: This approach helps target accidental repeat submissions or bot entries with minimal noise, especially in survey-based datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b832b-50c3-4a4d-9836-732479d3e18f",
   "metadata": {},
   "source": [
    "### Summary and Next Steps\n",
    "#### In this lab, you focused on identifying and analyzing duplicate rows within the dataset.\n",
    "\n",
    "#### You employed various techniques to explore the nature of duplicates and applied strategic methods for their removal.\n",
    "##### For additional analysis, consider investigating the impact of duplicates on specific analyses and how their removal affects the results.\n",
    "##### This version of the lab is more focused on duplicate analysis and handling, providing a structured approach to deal with duplicates in a dataset effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639fce7-b853-4a18-9d32-950a79e12553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
