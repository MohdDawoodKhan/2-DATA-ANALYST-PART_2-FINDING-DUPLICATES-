{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c11c4b-c32a-4359-96f4-eb37d2f73621",
   "metadata": {},
   "source": [
    "# Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775f84d-b47f-4e0c-bcda-f6ad3d8ede49",
   "metadata": {},
   "source": [
    "### Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bc3c3-a820-4821-88c6-2981406a4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5bb89-8bae-4da5-86ae-8b21ef9cc11d",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac42e15-d300-4f38-9394-5e7113aaf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40363e-d32b-4125-aa4c-c94ae9a6a428",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame¬∂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cad34-ecac-4210-8969-a79ac08e574e",
   "metadata": {},
   "source": [
    "#### load the dataset using pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05297218-e5b6-4340-97c5-cbe3bd97a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the dataset\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d5524-5280-4b46-8496-d1bb94257b6e",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ae0e6-f9ef-4e6d-8db4-be0bde70fdc0",
   "metadata": {},
   "source": [
    "##### Task 1: Identify Duplicate Rows\n",
    "\n",
    "##### 1-Count the number of duplicate rows in the dataset.\n",
    "##### 2-Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f2276-b5a5-49f4-bbcb-2ef8be4465e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1: Count duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Task 3.2: Display the first few duplicate rows\n",
    "duplicates = df[df.duplicated()]\n",
    "print(\"First few duplicate rows:\")\n",
    "print(duplicates.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fcc875-7dbb-4bf9-adc5-9651edcb6083",
   "metadata": {},
   "source": [
    "## Step 4: Removing Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c18d8-5367-4f11-8fc5-ba3e8c993522",
   "metadata": {},
   "source": [
    "#### Task 2: Remove Duplicates\n",
    "#### Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "#### Verify the removal by counting the number of duplicate rows after removal ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8cae6e-d4bb-4ac0-abcc-b25e18d9a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.1: Remove duplicate rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "print(\"Duplicate rows removed.\")\n",
    "\n",
    "# Task 4.2: Verify that duplicates are removed\n",
    "remaining_duplicates = df_cleaned.duplicated().sum()\n",
    "print(f\"Number of duplicate rows after removal: {remaining_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb22fd3-9da7-4fba-9aab-21ad4ef1f7ab",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec30fd0-0675-4641-9559-cb4662e994ac",
   "metadata": {},
   "source": [
    "#### Task 3: Identify and Handle Missing Values\n",
    "\n",
    "#### Identify missing values for all columns in the dataset.\n",
    "#### Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e72d40-a6fd-4e1a-9562-b4f0292af963",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "# Check how many missing values each column has\n",
    "missing_counts = df_cleaned.isnull().sum()\n",
    "print(\"Missing values per column:\\n\")\n",
    "print(missing_counts[missing_counts > 0])\n",
    "column_to_impute = 'EdLevel'  # Change if the column name is different in your dataset\n",
    "\n",
    "# Check the most frequent value (mode) of the column\n",
    "most_frequent = df_cleaned[column_to_impute].mode()[0]\n",
    "print(f\"\\nMost frequent value in '{column_to_impute}': {most_frequent}\")\n",
    "\n",
    "# Fill missing values in that column with the most frequent value\n",
    "df_cleaned[column_to_impute] = df_cleaned[column_to_impute].fillna(most_frequent)\n",
    "\n",
    "# Verify if missing values are handled\n",
    "missing_after_impute = df_cleaned[column_to_impute].isnull().sum()\n",
    "print(f\"\\nMissing values in '{column_to_impute}' after imputation: {missing_after_impute}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029b6df-471c-42b6-9470-91553dfb9408",
   "metadata": {},
   "source": [
    "## Step 6: Normalizing Compensation Data\n",
    "#### Task 4: Normalize Compensation Data Using ConvertedCompYearly\n",
    "\n",
    "#### Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "#### Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6aae2f-ea26-41c6-b3ba-fa952cfb13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "# Step 6.1: Check for missing values in ConvertedCompYearly\n",
    "missing_comp = df_cleaned['ConvertedCompYearly'].isnull().sum()\n",
    "print(f\"Missing values in 'ConvertedCompYearly': {missing_comp}\")\n",
    "if missing_comp > 0:\n",
    "    # Fill missing compensation with median (more robust than mean)\n",
    "    median_comp = df_cleaned['ConvertedCompYearly'].median()\n",
    "    df_cleaned['ConvertedCompYearly'] = df_cleaned['ConvertedCompYearly'].fillna(median_comp)\n",
    "    print(f\"Missing values filled with median: {median_comp}\")\n",
    "\n",
    "    # Confirm removal of missing values\n",
    "    print(\"Missing values after imputation:\", df_cleaned['ConvertedCompYearly'].isnull().sum())\n",
    "else:\n",
    "    print(\"No missing values in 'ConvertedCompYearly'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1fc3f-8170-40ed-8e5b-13167116c5e5",
   "metadata": {},
   "source": [
    "## Step 7: Summary and Next Step\n",
    "#### You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "#### You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "#### For further analysis, consider exploring other columns or visualizing the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e7274-de23-4e42-a13b-f84dada05fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîç Identified and Removed Duplicate Rows\n",
    "\n",
    "Used duplicated() and drop_duplicates() to ensure only unique records remain.\n",
    "\n",
    "‚ö†Ô∏è Handled Missing Values\n",
    "\n",
    "Displayed missing values in each column.\n",
    "\n",
    "Imputed missing values in the EdLevel column with the most frequent value (mode).\n",
    "\n",
    "üí∞ Used and Cleaned Compensation Data\n",
    "\n",
    "Focused on ConvertedCompYearly (normalized annual compensation).\n",
    "\n",
    "Filled missing compensation values with the median for robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
